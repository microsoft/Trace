{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Code AND Prompts with Trace\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to use the `trace` package to optimize prompts and code for natural language processing tasks using the BigBench-Hard benchmark. SotA approaches on this benchmark only optimize prompts, while relying on hand-written code to extract answers from LLM responses. By leveraging the LLM-based optimizers provided in `trace`, we aim to enhance the performance of a workflow calling LLMs and post-processing their responses in generating accurate and relevant answers.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll import the necessary packages and set up our environment. We will use a copy of the BigBench-Hard benchmark hosted on [HuggingFace](https://huggingface.co/datasets/maveriq/bigbenchhard). To use HuggingFace datasets, ensure that you have the `datasets` package installed:\n",
    "\n",
    "    git clone https://github.com/huggingface/datasets.git\n",
    "    cd datasets\n",
    "    pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:03.229950Z",
     "start_time": "2024-07-19T21:18:59.801910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import autogen\n",
    "from opto.trace.nodes import node, GRAPH, ParameterNode\n",
    "from opto.optimizers import FunctionOptimizerV2\n",
    "from datasets import load_dataset\n",
    "from textwrap import dedent\n",
    "from opto.trace.bundle import bundle\n",
    "from opto.trace.modules import model\n",
    "from opto.trace.errors import ExecutionError\n",
    "from opto.trace.nodes import ExceptionNode\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Evaluation Function\n",
    "\n",
    "Next, we'll define the utility function for evaluating answers obtained by prompting an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:03.230792Z",
     "start_time": "2024-07-19T21:19:03.223927Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_metric(true, prediction):\n",
    "    matches = re.findall(r\"\\([A-Z]\\)\", true)\n",
    "    if matches:\n",
    "        pred = prediction\n",
    "        matches = re.findall(r\"\\([A-Z]\\)\", pred)\n",
    "        parsed_answer = matches[-1] if matches else \"\"\n",
    "        return parsed_answer == true\n",
    "    else:\n",
    "        return prediction == true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "We'll create a helper class called `LLMCallable` to interact with the LLM API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:03.880915Z",
     "start_time": "2024-07-19T21:19:03.657068Z"
    }
   },
   "outputs": [],
   "source": [
    "class LLMCallable:\n",
    "    def __init__(self, config_list=None, max_tokens=1024, verbose=False):\n",
    "        if config_list is None:\n",
    "            config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n",
    "        self.llm = autogen.OpenAIWrapper(config_list=config_list)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @bundle(catch_execution_error=False)\n",
    "    def call_llm(self, user_prompt):\n",
    "        system_prompt = \"You are a helpful assistant.\\n\"\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        response = self.llm.create(messages=messages, max_tokens=self.max_tokens)\n",
    "        response = response.choices[0].message.content\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"LLM response:\\n\", response)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Traced Class\n",
    "\n",
    "We will define a Predict class to generate predictions using LLM. Note that we use a module provided by `trace` called `Model` which can wrap a python class to enable tracing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:04.970205Z",
     "start_time": "2024-07-19T21:19:04.933904Z"
    }
   },
   "outputs": [],
   "source": [
    "@model\n",
    "class Predict(LLMCallable):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.demos = []\n",
    "        self.prompt_template = dedent(\n",
    "            \"\"\"\n",
    "        Given the fields `question`, produce the fields `answer`.\n",
    "\n",
    "        ---\n",
    "\n",
    "        Follow the following format.\n",
    "\n",
    "        Question: \n",
    "        Answer: \n",
    "\n",
    "        ---\n",
    "        Question: {}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        )\n",
    "        self.prompt_template = ParameterNode(self.prompt_template, trainable=True,\n",
    "                                             description=\"[ParameterNode] This is the Prompt Template to the LLM. \" + \\\n",
    "                                                         \"Need to include information about what the format of answers LLM should output. \" + \\\n",
    "                                                         \"They can be (A)/(B), a number like 8, or a string, or Yes/No.\")\n",
    "\n",
    "    @bundle(trainable=True, catch_execution_error=True, allow_external_dependencies=True)\n",
    "    def extract_answer(self, prompt_template, question, response):\n",
    "        answer = response.split(\"Answer:\")[1].strip()\n",
    "        return answer\n",
    "\n",
    "    @bundle(trainable=True, catch_execution_error=True, allow_external_dependencies=True)\n",
    "    def create_prompt(self, prompt_template, question):\n",
    "        return prompt_template.format(question)\n",
    "\n",
    "    def forward(self, question):\n",
    "        user_prompt = self.create_prompt(self.prompt_template, question)\n",
    "        response = self.call_llm(user_prompt)\n",
    "        answer = self.extract_answer(self.prompt_template, question, response)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer\n",
    "\n",
    "Note that the `prompt_template` is a `ParameterNode` as well as the `extract_answer` is a trainable function. `trace` handles the optimization of heterogenous parameters seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:06.148198Z",
     "start_time": "2024-07-19T21:19:06.128481Z"
    }
   },
   "outputs": [],
   "source": [
    "def learn_predict(dp, optimizer, examples):\n",
    "    for step, example in enumerate(examples):\n",
    "        GRAPH.clear()\n",
    "        try:\n",
    "            response = dp.forward(example['question'])\n",
    "            correctness = eval_metric(example['answer'], response)\n",
    "            feedback = \"The answer is correct! No need to change anything.\" if correctness else f\"The answer is wrong. We expect the output of your answer to be \\\"{example['answer']}\\\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\"\n",
    "        except ExecutionError as e:\n",
    "            response = e.exception_node\n",
    "            feedback = response.data\n",
    "            correctness = False\n",
    "            \n",
    "        print(\"Question:\", example[\"question\"])\n",
    "        print(\"Expected answer:\", example[\"answer\"])\n",
    "        print(\"Answer:\", response)\n",
    "\n",
    "        if correctness:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(response, feedback)\n",
    "\n",
    "        print(f\"Output: {response}, Feedback: {feedback}, Variables:\")  # Logging\n",
    "        for p in optimizer.parameters:\n",
    "            print(p.name, p.data)\n",
    "        optimizer.step(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Finally, we use the optimizer to find better prompts using a small training set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T21:19:21.867979Z",
     "start_time": "2024-07-19T21:19:07.382684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a few examples:\n",
      "Question: Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\n",
      "Expected answer: no\n",
      "Answer: MessageNode: (eval:1, dtype=<class 'str'>, data=Yes, the sentence \"Elias Lindholm beat the buzzer.\" is plausible. It suggests that Elias Lindholm, likely a sports player, scored or accomplished something significant just before a deadline or timer expired in a game.)\n",
      "Output: MessageNode: (eval:1, dtype=<class 'str'>, data=Yes, the sentence \"Elias Lindholm beat the buzzer.\" is plausible. It suggests that Elias Lindholm, likely a sports player, scored or accomplished something significant just before a deadline or timer expired in a game.), Feedback: The answer is wrong. We expect the output of your answer to be \"no\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer., Variables:\n",
      "__code:1 def create_prompt(self, prompt_template, question):\n",
      "        return prompt_template.format(question)\n",
      "__code:0 def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "str:1 \n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: \n",
      "Answer: \n",
      "\n",
      "---\n",
      "Question: {}\n",
      "Answer:\n",
      "\n",
      "str:1 \n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: \n",
      "Answer: \n",
      "\n",
      "---\n",
      "Question: {}\n",
      "Answer:\n",
      "\n",
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "In \"reasoning\", explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Output means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If #Instruction asks for an answer, write it down in \"answer\".\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "eval0 = eval(self=ModelWrapper0, prompt_template=str1, question=str0, __code=__code1)\n",
      "LLMCallable.call_llm0 = LLMCallable.call_llm(self=ModelWrapper1, user_prompt=eval0)\n",
      "\n",
      "#Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "[LLMCallable.call_llm] .\n",
      "\n",
      "#Variables\n",
      "(str) str1=\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: \n",
      "Answer: \n",
      "\n",
      "---\n",
      "Question: {}\n",
      "Answer:\n",
      "\n",
      "(code) __code1:def create_prompt(self, prompt_template, question):\n",
      "        return prompt_template.format(question)\n",
      "(code) __code0:def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "\n",
      "#Constraints\n",
      "(code) __code1: The code should start with:\n",
      "def create_prompt(self, prompt_template, question):\n",
      "(code) __code0: The code should start with:\n",
      "def extract_answer(self, prompt_template, question, response):\n",
      "\n",
      "#Inputs\n",
      "(ModelWrapper) ModelWrapper2=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper1=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper0=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(str) str0=Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\n",
      "(str) eval1=Yes, the sentence \"Elias Lindholm beat the buzzer.\" is plausible. It suggests that Elias Lindholm, likely a sports player, scored or accomplished something significant just before a deadline or timer expired in a game.\n",
      "\n",
      "#Others\n",
      "(str) eval0=\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: \n",
      "Answer: \n",
      "\n",
      "---\n",
      "Question: Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\n",
      "Answer:\n",
      "\n",
      "(str) LLMCallable.call_llm0=Question: Is the following sentence plausible? \"Elias Lindholm beat the buzzer.\"\n",
      "Answer: Yes, the sentence \"Elias Lindholm beat the buzzer.\" is plausible. It suggests that Elias Lindholm, likely a sports player, scored or accomplished something significant just before a deadline or timer expired in a game.\n",
      "\n",
      "#Outputs\n",
      "\n",
      "\n",
      "#Feedback:\n",
      "The answer is wrong. We expect the output of your answer to be \"no\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "  \"reasoning\": \"The task is to improve the output based on the feedback provided, where 'no' is expected as the answer rather than 'yes'. Looking at the existing code and templates, the issue likely originates from the prompt formatting and instructions given to the LLM for generating an answer. Currently, the prompt given to the LLM consists of an empty template formatted to include the 'str0' question. However, it is evident that the formatting and guidance within the prompt template are insufficient or incorrectly guiding the LLM to deliver the desired answer ('no'). Modifying the prompt template (str1) to explicitly guide the model on assessing the plausibility factually rather than inferring general plausibility based on context will likely yield better results. Adjusting the prompt to explicitly guide towards a factual assessment of plausibility promotes a straightforward analysis by the LLM, hence increasing the likelihood of receiving the correct answer.\",\n",
      "  \"answer\": \"\",\n",
      "  \"suggestion\": {\n",
      "    \"str1\": \"Given the fields `question`, produce the fields `answer`\\\\n\\\\n---\\\\n\\\\nFollow the fact-checking format and validate information preciseness.\\\\n\\\\nQuestion: {question}\\\\nAnswer: \"\n",
      "  }\n",
      "}\n",
      "Question: Is the following sentence plausible? \"John Carlson scored in the third period.\"\n",
      "Expected answer: yes\n",
      "Answer: MessageNode: (exception_eval:0, dtype=<class 'str'>, data=(KeyError) 'question')\n",
      "Output: MessageNode: (exception_eval:0, dtype=<class 'str'>, data=(KeyError) 'question'), Feedback: (KeyError) 'question', Variables:\n",
      "__code:1 def create_prompt(self, prompt_template, question):\n",
      "        return prompt_template.format(question)\n",
      "__code:0 def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "str:1 Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "str:1 Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "In \"reasoning\", explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Output means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If #Instruction asks for an answer, write it down in \"answer\".\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "exception_eval0 = eval(self=ModelWrapper0, prompt_template=str1, question=str0, __code=__code1)\n",
      "\n",
      "#Documentation\n",
      "[exception] The operator eval raises an exception.\n",
      "\n",
      "#Variables\n",
      "(str) str1=Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "(code) __code1:def create_prompt(self, prompt_template, question):\n",
      "        return prompt_template.format(question)\n",
      "\n",
      "#Constraints\n",
      "(code) __code1: The code should start with:\n",
      "def create_prompt(self, prompt_template, question):\n",
      "\n",
      "#Inputs\n",
      "(ModelWrapper) ModelWrapper0=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(str) str0=Is the following sentence plausible? \"John Carlson scored in the third period.\"\n",
      "\n",
      "#Others\n",
      "\n",
      "\n",
      "#Outputs\n",
      "(str) exception_eval0=(KeyError) 'question'\n",
      "\n",
      "#Feedback:\n",
      "(KeyError) 'question'\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "  \"reasoning\": \"The KeyError 'question' arises due to the inappropriate use of the eval function in the provided code. The eval function is attempting to execute the function create_prompt using syntax where `self`, `prompt_template`, and `question` are named explicitly in the eval call with the `__code` parameter, which seems to be incorrect or not supported in the current context. From the documentation and the error output, it appears that kwargs like `self=ModelWrapper0` or `prompt_template=str1` are causing the function to not recognize or improperly handle the 'question' key passed to the create_prompt function. This misconfiguration in the eval function leads to failure in string formatting, as the key 'question' does not exist in the function's execution context leading to the KeyError. Adjusting the __code1 to use proper positional or keyword arguments in the function or altering its calling convention is necessary.\",\n",
      "  \"answer\": \"\",\n",
      "  \"suggestion\": {\n",
      "    \"__code1\": \"def create_prompt(self, prompt_template, question):\\n\\t\\treturn prompt_template.format(question=question)\"\n",
      "  }\n",
      "}\n",
      "Question: Is the following sentence plausible? \"Marcelo got on the end of a through ball.\"\n",
      "Expected answer: yes\n",
      "Answer: MessageNode: (eval:1, dtype=<class 'str'>, data=Yes, the sentence \"Marcelo got on the end of a through ball\" is plausible. The phrase \"got on the end of a through ball\" typically refers to a soccer scenario where a player reaches a pass that has been sent through the opponents' defensive line, potentially putting the player in a good position to score or create a scoring opportunity. Marcelo, being a common soccer player's name, fits well within this context. Hence, the sentence makes sense in a football (soccer) setting.)\n",
      "Output: MessageNode: (eval:1, dtype=<class 'str'>, data=Yes, the sentence \"Marcelo got on the end of a through ball\" is plausible. The phrase \"got on the end of a through ball\" typically refers to a soccer scenario where a player reaches a pass that has been sent through the opponents' defensive line, potentially putting the player in a good position to score or create a scoring opportunity. Marcelo, being a common soccer player's name, fits well within this context. Hence, the sentence makes sense in a football (soccer) setting.), Feedback: The answer is wrong. We expect the output of your answer to be \"yes\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer., Variables:\n",
      "__code:1 def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "__code:0 def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "str:1 Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "str:1 Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "In \"reasoning\", explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Output means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If #Instruction asks for an answer, write it down in \"answer\".\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "eval0 = eval(self=ModelWrapper0, prompt_template=str1, question=str0, __code=__code1)\n",
      "LLMCallable.call_llm0 = LLMCallable.call_llm(self=ModelWrapper1, user_prompt=eval0)\n",
      "\n",
      "#Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "[LLMCallable.call_llm] .\n",
      "\n",
      "#Variables\n",
      "(str) str1=Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: {question}\\nAnswer: \n",
      "(code) __code1:def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "(code) __code0:def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "\n",
      "#Constraints\n",
      "(code) __code1: The code should start with:\n",
      "def create_prompt(self, prompt_template, question):\n",
      "(code) __code0: The code should start with:\n",
      "def extract_answer(self, prompt_template, question, response):\n",
      "\n",
      "#Inputs\n",
      "(ModelWrapper) ModelWrapper2=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper1=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper0=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(str) str0=Is the following sentence plausible? \"Marcelo got on the end of a through ball.\"\n",
      "(str) eval1=Yes, the sentence \"Marcelo got on the end of a through ball\" is plausible. The phrase \"got on the end of a through ball\" typically refers to a soccer scenario where a player reaches a pass that has been sent through the opponents' defensive line, potentially putting the player in a good position to score or create a scoring opportunity. Marcelo, being a common soccer player's name, fits well within this context. Hence, the sentence makes sense in a football (soccer) setting.\n",
      "\n",
      "#Others\n",
      "(str) eval0=Given the fields `question`, produce the fields `answer`\\n\\n---\\n\\nFollow the fact-checking format and validate information preciseness.\\n\\nQuestion: Is the following sentence plausible? \"Marcelo got on the end of a through ball.\"\\nAnswer: \n",
      "(str) LLMCallable.call_llm0=Answer: Yes, the sentence \"Marcelo got on the end of a through ball\" is plausible. The phrase \"got on the end of a through ball\" typically refers to a soccer scenario where a player reaches a pass that has been sent through the opponents' defensive line, potentially putting the player in a good position to score or create a scoring opportunity. Marcelo, being a common soccer player's name, fits well within this context. Hence, the sentence makes sense in a football (soccer) setting.\n",
      "\n",
      "#Outputs\n",
      "\n",
      "\n",
      "#Feedback:\n",
      "The answer is wrong. We expect the output of your answer to be \"yes\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "    \"reasoning\": \"The task is to modify the variables such that the large language model (LLM) produces the correct answer format, namely 'yes' to indicate plausibility, instead of a detailed explanation. Looking at the current setup: \\n\\n1. The `str1` variable is used to create a structured prompt that includes instructions for fact-checking and preciseness, and then formulating a question-answer format. The current format in `str1` inherently encourages a detailed response because it explicitly asks for a validation of information's preciseness, which might have influenced the detailed explanation from the LLM.\\n\\n2. The `__code1` function formats the prompt to include this question, correctly formatted per `str1`. Given this setup, a straightforward 'yes' or 'no' response might be impossible to achieve unless the initial prompt (defined in `str1`) is altered to specifically ask for such a response. Therefore, modifying `str1` to a simpler question format and explicitly requesting a binary (yes/no) answer should guide the LLM better.\\n\\n3. No changes to `__code0` are suggested as it seems to function correctly in extracting the part of the LLM's response following 'Answer:', although it is not actively used in the provided code excerpts.\",\n",
      "    \"answer\": \"\",\n",
      "    \"suggestion\": {\n",
      "        \"str1\": \"Is the following statement plausible? Answer with 'yes' or 'no' only.\\n\\nQuestion: {question}\"\n",
      "    }\n",
      "}\n",
      "Question: Is the following sentence plausible? \"Deshaun Watson was called for the goal tend in the Eastern Conference Finals.\"\n",
      "Expected answer: no\n",
      "Answer: MessageNode: (exception_eval:0, dtype=<class 'str'>, data=(IndexError) list index out of range)\n",
      "Output: MessageNode: (exception_eval:0, dtype=<class 'str'>, data=(IndexError) list index out of range), Feedback: (IndexError) list index out of range, Variables:\n",
      "__code:1 def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "__code:0 def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "str:1 Is the following statement plausible? Answer with 'yes' or 'no' only.\n",
      "\n",
      "Question: {question}\n",
      "str:1 Is the following statement plausible? Answer with 'yes' or 'no' only.\n",
      "\n",
      "Question: {question}\n",
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "In \"reasoning\", explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Output means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If #Instruction asks for an answer, write it down in \"answer\".\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "eval0 = eval(self=ModelWrapper0, prompt_template=str1, question=str0, __code=__code1)\n",
      "LLMCallable.call_llm0 = LLMCallable.call_llm(self=ModelWrapper1, user_prompt=eval0)\n",
      "\n",
      "#Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "[LLMCallable.call_llm] .\n",
      "\n",
      "#Variables\n",
      "(str) str1=Is the following statement plausible? Answer with 'yes' or 'no' only.\n",
      "\n",
      "Question: {question}\n",
      "(code) __code1:def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "(code) __code0:def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "\n",
      "#Constraints\n",
      "(code) __code1: The code should start with:\n",
      "def create_prompt(self, prompt_template, question):\n",
      "(code) __code0: The code should start with:\n",
      "def extract_answer(self, prompt_template, question, response):\n",
      "\n",
      "#Inputs\n",
      "(ModelWrapper) ModelWrapper2=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper1=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper0=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(str) str0=Is the following sentence plausible? \"Deshaun Watson was called for the goal tend in the Eastern Conference Finals.\"\n",
      "(str) exception_eval0=(IndexError) list index out of range\n",
      "\n",
      "#Others\n",
      "(str) eval0=Is the following statement plausible? Answer with 'yes' or 'no' only.\n",
      "\n",
      "Question: Is the following sentence plausible? \"Deshaun Watson was called for the goal tend in the Eastern Conference Finals.\"\n",
      "(str) LLMCallable.call_llm0=No.\n",
      "\n",
      "#Outputs\n",
      "\n",
      "\n",
      "#Feedback:\n",
      "(IndexError) list index out of range\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "    \"reasoning\": \"The IndexError 'list index out of range' in the feedback refers to an issue occurring somewhere involving list access. Upon examining the functions and structures provided, the first thing to notice is that the eval function successfully creates a formatted prompt without issues, indicated by 'eval0' being properly set in #Others. The error must then be happening after this point, which would involve the response being generated and then parsed.\\n\\nLooking at the __code0 function for 'extract_answer', the function attempts to split the response on 'Answer:' and take the second element from this split operation. The error comes from the fact that the response returned, 'No.', does't contain the word 'Answer:' leading to the split result being a list of length 1 and subsequently accessing the second element [1] is causing the IndexError.\\n\\nConsidering this, the proper rectification would involve adjusting the output expectation within the LLM responses to match the format expected by 'extract_answer'. This means that the generation performed by LLM needs to include the word 'Answer:' in its response, which according to the provided code, happens before the 'extract_answer' function is used. However, making changes to that directly isn't an option here due to constraints. Thus, the available modification lies in the prompt template 'str1' to possibly cue the LLM towards the expected response format.\",\n",
      "    \"suggestion\": {\n",
      "        \"str1\": \"Is the following statement plausible? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\\\n\\\\nQuestion: {question}\"\n",
      "    }\n",
      "}\n",
      "Question: Is the following sentence plausible? \"Mookie Betts skated behind the net.\"\n",
      "Expected answer: no\n",
      "Answer: MessageNode: (eval:1, dtype=<class 'str'>, data=yes)\n",
      "Output: MessageNode: (eval:1, dtype=<class 'str'>, data=yes), Feedback: The answer is wrong. We expect the output of your answer to be \"no\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer., Variables:\n",
      "__code:1 def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "__code:0 def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "str:1 Is the following statement plausible? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\n\\nQuestion: {question}\n",
      "str:1 Is the following statement plausible? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\n\\nQuestion: {question}\n",
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "In \"reasoning\", explain the problem: 1. what the #Instruction means 2. what the #Feedback on #Output means to #Variables considering how #Variables are used in #Code and other values in #Documentation, #Inputs, #Others. 3. Reasoning about the suggested changes in #Variables (if needed) and the expected result.\n",
      "\n",
      "If #Instruction asks for an answer, write it down in \"answer\".\n",
      "\n",
      "If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "eval0 = eval(self=ModelWrapper0, prompt_template=str1, question=str0, __code=__code1)\n",
      "LLMCallable.call_llm0 = LLMCallable.call_llm(self=ModelWrapper1, user_prompt=eval0)\n",
      "\n",
      "#Documentation\n",
      "[eval] This operator eval(__code, *args, **kwargs) evaluates the code block, where __code is the code (str) and *args and **kwargs are the arguments of the function. The output is the result of the evaluation, i.e., __code(*args, **kwargs).\n",
      "[LLMCallable.call_llm] .\n",
      "\n",
      "#Variables\n",
      "(str) str1=Is the following statement plausible? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\n\\nQuestion: {question}\n",
      "(code) __code1:def create_prompt(self, prompt_template, question):\n",
      "\t\treturn prompt_template.format(question=question)\n",
      "(code) __code0:def extract_answer(self, prompt_template, question, response):\n",
      "        answer = response.split(\"Answer:\")[1].strip()\n",
      "        return answer\n",
      "\n",
      "#Constraints\n",
      "(code) __code1: The code should start with:\n",
      "def create_prompt(self, prompt_template, question):\n",
      "(code) __code0: The code should start with:\n",
      "def extract_answer(self, prompt_template, question, response):\n",
      "\n",
      "#Inputs\n",
      "(ModelWrapper) ModelWrapper2=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper1=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(ModelWrapper) ModelWrapper0=<opto.trace.modules.model.<locals>.ModelWrapper object at 0x7fd3b0f34d90>\n",
      "(str) str0=Is the following sentence plausible? \"Mookie Betts skated behind the net.\"\n",
      "(str) eval1=yes\n",
      "\n",
      "#Others\n",
      "(str) eval0=Is the following statement plausible? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\n\\nQuestion: Is the following sentence plausible? \"Mookie Betts skated behind the net.\"\n",
      "(str) LLMCallable.call_llm0=Answer: yes\n",
      "\n",
      "#Outputs\n",
      "\n",
      "\n",
      "#Feedback:\n",
      "The answer is wrong. We expect the output of your answer to be \"no\". Please modify the prompt and relevant parts of the program to help LLM produce the right answer.\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "  \"reasoning\": \"The task at hand involves modifying certain elements of the prompt to guide the Large Language Model (LLM) to answer 'no' rather than 'yes' to the statement about Mookie Betts, who is known primarily for his skills in baseball, not ice hockey. Currently, the prompt provided to the LLM reads: 'Is the following statement plausible?' followed by the question concerning Mookie Betts skating. This is neutral and does not guide the LLM toward understanding why the question might be implausible given Mookie Betts' actual sporting background. Adjusting the wording in the prompt to not only inquire about the plausibility of the statement but to also implicitly point towards the oddity of the statement (mentioning his actual sports) could guide the LLM to produce the expected 'no' answer.\",\n",
      "  \"suggestion\": {\n",
      "    \"str1\": \"Is the following statement plausible given that Mookie Betts is primarily known as a baseball player for the Los Angeles Dodgers? Please prefix your answer with 'Answer: ' and reply with 'yes' or 'no' only.\\n\\nQuestion: {question}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Testing on new examples:\n",
      "Question: Is the following sentence plausible? \"John Tavares earned a trip to the penalty box in the Stanley Cup.\"\n",
      "Expected answer: yes\n",
      "Answer: MessageNode: (eval:3, dtype=<class 'str'>, data=yes)\n"
     ]
    }
   ],
   "source": [
    "task = \"sports_understanding\"\n",
    "train = load_dataset(\"maveriq/bigbenchhard\", task)[\"train\"]\n",
    "examples = [{\"question\": r[\"input\"], \"answer\": r[\"target\"]} for r in train]\n",
    "\n",
    "dp = Predict()\n",
    "optimizer = FunctionOptimizerV2(dp.parameters() + [dp.prompt_template],\n",
    "                                    config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "print(\"Training on a few examples:\")\n",
    "learn_predict(dp, optimizer, examples[:5])\n",
    "    \n",
    "print(\"\\nTesting on new examples:\")\n",
    "for example in examples[5:6]:\n",
    "    response = dp.forward(example[\"question\"])\n",
    "    print(\"Question:\", example[\"question\"])\n",
    "    print(\"Expected answer:\", example[\"answer\"])\n",
    "    print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can run each cell in this notebook step by step to walk through the process of setting up and optimizing prompts for the trading game. Happy optimizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verbal-gym",
   "language": "python",
   "name": "verbal-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
