{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Here we show a small example of how to apply `trace` to optimize python objects based on language feedback. Here we want to change the input to function `foobar` such that output is large enough. `foobar` is a function that is composed of `foo` based on built-in operators and `bar` which is a blackbox function, whose information is only given via the docstring.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opto\n",
    "from opto.trace import bundle, node\n",
    "from opto.optimizers import FunctionOptimizer\n",
    "from opto.trace.nodes import GRAPH\n",
    "\n",
    "\n",
    "def blackbox(x):\n",
    "    return -x * 2\n",
    "\n",
    "\n",
    "@bundle()\n",
    "def bar(x):\n",
    "    \"This is a test function, which does negative scaling.\"\n",
    "    return blackbox(x)\n",
    "\n",
    "\n",
    "def foo(x):\n",
    "    y = x + 1\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# foobar is a composition of custom function and built-in functions\n",
    "\n",
    "\n",
    "def foobar(x):\n",
    "    return foo(bar(x))\n",
    "\n",
    "\n",
    "def user(x):\n",
    "    if x < 50:\n",
    "        return \"The number needs to be larger.\"\n",
    "    else:\n",
    "        return \"Success.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "We apply `FunctionOptimizer` to change the input to the function `foobar` such that the simulated user is satisfied. To this end, we backpropagated the user's language feedback about the output, through the graph that connects the input to the output.\n",
    "\n",
    "We use helper functions from [AutoGen](https://github.com/microsoft/autogen) to call LLMs to interpret the user's language feedback. Before running the cell below, please copy `OAI_CONFIG_LIST_sample` from the root folder of this repository to the current folder, rename it to `OAI_CONFIG_LIST`, and set the correct configuration for LLMs in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"888pt\" height=\"394pt\"\n",
       " viewBox=\"0.00 0.00 888.34 393.86\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 389.86)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-389.86 884.34,-389.86 884.34,4 -4,4\"/>\n",
       "<!-- bar0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>bar0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"645.58\" cy=\"-259.38\" rx=\"234.52\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-270.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">bar0</text>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-255.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[bar] This is a test function, which does negative scaling..</text>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-240.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2.0</text>\n",
       "</g>\n",
       "<!-- multiply0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>multiply0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"545.58\" cy=\"-37.48\" rx=\"205.12\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"545.58\" y=\"-48.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">multiply0</text>\n",
       "<text text-anchor=\"middle\" x=\"545.58\" y=\"-33.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[multiply] This is a multiply operator of x and y. .</text>\n",
       "<text text-anchor=\"middle\" x=\"545.58\" y=\"-18.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">6.0</text>\n",
       "</g>\n",
       "<!-- bar0&#45;&gt;multiply0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>bar0&#45;&gt;multiply0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M647.77,-221.73C648.08,-190.53 644.73,-145.44 626.58,-110.95 620.82,-100.02 612.71,-89.93 603.82,-80.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"606.21,-78.42 596.56,-74.05 601.38,-83.49 606.21,-78.42\"/>\n",
       "</g>\n",
       "<!-- add0 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>add0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"445.58\" cy=\"-148.43\" rx=\"171.65\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"445.58\" y=\"-159.73\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">add0</text>\n",
       "<text text-anchor=\"middle\" x=\"445.58\" y=\"-144.73\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[add] This is an add operator of x and y. .</text>\n",
       "<text text-anchor=\"middle\" x=\"445.58\" y=\"-129.73\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3.0</text>\n",
       "</g>\n",
       "<!-- bar0&#45;&gt;add0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>bar0&#45;&gt;add0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M581.19,-223.31C560.71,-212.15 537.93,-199.74 516.98,-188.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"518.59,-185.22 508.13,-183.51 515.24,-191.37 518.59,-185.22\"/>\n",
       "</g>\n",
       "<!-- add0&#45;&gt;multiply0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>add0&#45;&gt;multiply0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M478.6,-111.45C487.13,-102.16 496.4,-92.05 505.26,-82.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"508.06,-84.52 512.25,-74.79 502.91,-79.79 508.06,-84.52\"/>\n",
       "</g>\n",
       "<!-- int0 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>int0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"196.58\" cy=\"-259.38\" rx=\"196.65\" ry=\"37.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.58\" y=\"-270.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">int0</text>\n",
       "<text text-anchor=\"middle\" x=\"196.58\" y=\"-255.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[Node] This is a node in a computational graph.</text>\n",
       "<text text-anchor=\"middle\" x=\"196.58\" y=\"-240.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- int0&#45;&gt;add0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>int0&#45;&gt;add0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M273.32,-224.8C301.05,-212.67 332.5,-198.91 360.88,-186.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.61,-189.55 370.37,-182.34 359.81,-183.14 362.61,-189.55\"/>\n",
       "</g>\n",
       "<!-- float1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>float1</title>\n",
       "<polygon fill=\"lightgray\" stroke=\"black\" points=\"841.58,-385.86 449.58,-385.86 449.58,-332.86 841.58,-332.86 841.58,-385.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-370.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">float1</text>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-355.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">[ParameterNode] This is a ParameterNode in a computational graph.</text>\n",
       "<text text-anchor=\"middle\" x=\"645.58\" y=\"-340.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">&#45;1.0</text>\n",
       "</g>\n",
       "<!-- float1&#45;&gt;bar0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>float1&#45;&gt;bar0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M645.58,-332.62C645.58,-324.79 645.58,-315.94 645.58,-307.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"649.08,-307.01 645.58,-297.01 642.08,-307.01 649.08,-307.01\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x21b7d117f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "# One-step optimization example\n",
    "x = node(-1.0, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "output = foobar(x)\n",
    "feedback = user(output.data)\n",
    "optimizer.zero_feedback()\n",
    "optimizer.backward(output, feedback, visualize=True)  # this is equivalent to the line below\n",
    "# output.backward(feedback, propagator=optimizer.propagator, visualize=visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propagated feedback contains graph structure, data of the nodes in the graph, and the transformation used in the graph. They are presented in a python-like syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Feedback\n",
      "Graph:\n",
      "  1: bar0 = bar(x=float1)\n",
      "  2: add0 = add(x=bar0, y=int0)\n",
      "  3: multiply0 = multiply(x=bar0, y=add0)\n",
      "Roots:\n",
      "  float1: (-1.0, None)\n",
      "  int0: (1, None)\n",
      "Others:\n",
      "  bar0: (2.0, None)\n",
      "  add0: (3.0, None)\n",
      "Documentation:\n",
      "  bar: [bar] This is a test function, which does negative scaling..\n",
      "  add: [add] This is an add operator of x and y. .\n",
      "  multiply: [multiply] This is a multiply operator of x and y. .\n",
      "Output:\n",
      "  multiply0: (6.0, None)\n",
      "User Feedback:\n",
      "  The number needs to be larger.\n"
     ]
    }
   ],
   "source": [
    "from opto.optimizers.function_optimizer import node_to_function_feedback\n",
    "\n",
    "print(\"Function Feedback\")\n",
    "for k, v in x.feedback.items():\n",
    "    v = v[0]\n",
    "    f_feedback = node_to_function_feedback(v)\n",
    "    print(\"Graph:\")\n",
    "    for kk, vv in f_feedback.graph:\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Roots:\")\n",
    "    for kk, vv in f_feedback.roots.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Others:\")\n",
    "    for kk, vv in f_feedback.others.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Documentation:\")\n",
    "    for kk, vv in f_feedback.documentation.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Output:\")\n",
    "    for kk, vv in f_feedback.output.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"User Feedback:\")\n",
    "    print(f\"  {f_feedback.user_feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the feedback is propagated, we can call the optimizer to change the variable based on the feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt\n",
      " \n",
      "You're tasked to solve a coding/algorithm problem. You will see the instruction, the code, the documentation of each function used in the code, and the feedback about the execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Instruction: the instruction which describes the things you need to do or the question you should answer.\n",
      "- #Code: the code defined in the problem.\n",
      "- #Documentation: the documentation of each function used in #Code. The explanation might be incomplete and just contain high-level description. You can use the values in #Others to help infer how those functions work.\n",
      "- #Variables: the input variables that you can change.\n",
      "- #Constraints: the constraints or descriptions of the variables in #Variables.\n",
      "- #Inputs: the values of other inputs to the code, which are not changeable.\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code output.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Inputs, #Outputs, and #Others, the format is:\n",
      "\n",
      "<data_type> <variable_name> = <value>\n",
      "\n",
      "If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "\n",
      "Output_format: Your output should be in the following json format, satisfying the json syntax:\n",
      "\n",
      "{{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"answer\": <Your answer>,\n",
      "\"suggestion\": {{\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}}\n",
      "}}\n",
      "\n",
      "You should write down your thought process in \"reasoning\". If #Instruction asks for an answer, write it down in \"answer\". If you need to suggest a change in the values of #Variables, write down the suggested values in \"suggestion\". Remember you can change only the values in #Variables, not others. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors, and you should not change the function name or the function signature.\n",
      "\n",
      "If no changes or answer are needed, just output TERMINATE.\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "#Instruction\n",
      "You need to change the <value> of the variables in #Variables to improve the output in accordance to #Feedback.\n",
      "\n",
      "#Code\n",
      "bar0 = bar(x=float1)\n",
      "add0 = add(x=bar0, y=int0)\n",
      "multiply0 = multiply(x=bar0, y=add0)\n",
      "\n",
      "#Documentation\n",
      "[bar] This is a test function, which does negative scaling..\n",
      "[add] This is an add operator of x and y. .\n",
      "[multiply] This is a multiply operator of x and y. .\n",
      "\n",
      "#Variables\n",
      "(float) float1=-1.0\n",
      "\n",
      "#Constraints\n",
      "\n",
      "\n",
      "#Inputs\n",
      "(int) int0=1\n",
      "\n",
      "#Others\n",
      "(float) bar0=2.0\n",
      "(float) add0=3.0\n",
      "\n",
      "#Outputs\n",
      "(float) multiply0=6.0\n",
      "\n",
      "#Feedback:\n",
      "The number needs to be larger.\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "Your response:\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback that the number needs to be larger, we need to adjust the variables to ensure the output of 'multiply0' increases. The operation follows a sequence where first, 'float1' is used by the 'bar' function, which apparently scales the number (despite the documentation mentioning negative scaling, 'float1' of -1.0 leading to 'bar0' of 2.0 indicates either positive scaling or an incorrect documentation). Then, 'add0' adds 'bar0' and 'int0' (1), resulting in 'add0'=3.0. Finally, 'multiply' multiplies 'bar0' with 'add0', resulting in 'multiply0'=6.0. To increase 'multiply0', we can manipulate 'float1', which is the initial value that impacts all subsequent operations directly or indirectly. Increasing its magnitude should, based on the provided operations and their results, increase the output of 'multiply0', assuming 'bar' does indeed scale positively in this specific instance.\",\n",
      "\"answer\": \"\",\n",
      "\"suggestion\": {\n",
      "    \"float1\": \"-2.0\"\n",
      "}\n",
      "}\n",
      "\n",
      "After step\n",
      "old variable -1.0\n",
      "new variable -2.0\n"
     ]
    }
   ],
   "source": [
    "old_variable = x.data\n",
    "optimizer.step(verbose=True)\n",
    "\n",
    "print(\"\\nAfter step\")\n",
    "print(\"old variable\", old_variable)\n",
    "print(\"new variable\", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Full Optimization Loop\n",
    "\n",
    "We can apply the steps above repeatedly to create a training loop to optimize the variable according to the user. Notice because of the way `foobar` works, the optimizer actually needs to change the input to be lower in order to make the output to be larger (which is what the user suggests). \n",
    "\n",
    "This is a non-trivial problem, becasue the optimizer sees only\n",
    "\n",
    "```\n",
    "output = blackbox(x) * (blackbox(x)+1)\n",
    "```\n",
    "\n",
    "and the hint/docstring `\"This is a test function, which does scaling and negation.\"` about how `blackbox` works. The optimizer needs to figure out how to change the input based on this vague information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=-1.0, output=6.0, feedback=The number needs to be larger.\n",
      "variable=2.0, output=12.0, feedback=The number needs to be larger.\n",
      "variable=4.0, output=56.0, feedback=Success.\n",
      "Cannot extract suggestion from LLM's response:\n",
      "{\"reasoning\": \"Since the feedback indicates success, there is no need to change the value of 'float0'. The flow of operations and the output suggest that the operations bar, add, and multiply have been executed correctly and produced the expected outcome based on the given inputs and functions.\", \"answer\": \"No changes needed\", \"suggestion\": {}}\n",
      "History\n",
      "  0: -1.0\n",
      "  1: 2.0\n",
      "  2: 4.0\n",
      "  3: 4.0\n"
     ]
    }
   ],
   "source": [
    "# A small example of how to use the optimizer in a loop\n",
    "GRAPH.clear()\n",
    "x = node(-1.0, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = [x.data]\n",
    "feedback = \"\"\n",
    "while feedback.lower() != \"Success.\".lower():\n",
    "    output = foobar(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "    print(f\"variable={x.data}, output={output.data}, feedback={feedback}\")  # logging\n",
    "    optimizer.step()\n",
    "    history.append(x.data)  # logging\n",
    "\n",
    "print(\"History\")\n",
    "for i, v in enumerate(history):\n",
    "    print(f\"  {i}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding constraints\n",
    "\n",
    "We can add constraints to parameter nodes to guide the optimizer. In this small example, the constraint info helps save one optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=-1.0, output=6.0, feedback=The number needs to be larger.\n",
      "variable=4.0, output=56.0, feedback=Success.\n",
      "Cannot extract suggestion from LLM's response:\n",
      "{\n",
      "\"reasoning\": \"Since the feedback indicates success and all the operations performed in the code correctly output the result based on the provided functions and inputs, there is no need to make any changes to the variables. The bar function correctly transforms the input float by applying negative scaling, leading to `bar1 = -8.0` when `float0 = 4.0`. The add function then adds `bar1` and `int1`, resulting in `add1 = -7.0`. Lastly, the multiply function multiplies `bar1` by `add1`, resulting in `multiply1 = 56.0`. All these operations align with the described behavior of the functions and meet the requirement as indicated in the feedback.\",\n",
      "\"answer\": \"No changes needed.\",\n",
      "\"suggestion\": {}\n",
      "}\n",
      "History\n",
      "  0: -1.0\n",
      "  1: 4.0\n",
      "  2: 4.0\n"
     ]
    }
   ],
   "source": [
    "# A small example of how to include constraints on parameters\n",
    "GRAPH.clear()\n",
    "x = node(-1.0, trainable=True, constraint=\"The value should be greater than 2.0\")\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = [x.data]\n",
    "feedback = \"\"\n",
    "while feedback.lower() != \"Success.\".lower():\n",
    "    output = foobar(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "    print(f\"variable={x.data}, output={output.data}, feedback={feedback}\")  # logging\n",
    "    optimizer.step()\n",
    "    history.append(x.data)  # logging\n",
    "\n",
    "print(\"History\")\n",
    "for i, v in enumerate(history):\n",
    "    print(f\"  {i}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of optimizing strings\n",
    "\n",
    "Below is a similar example, except the variable is written in text and is converted by a poor converter to numbers before inputting to `foo` and `bar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bundle()\n",
    "def convert_english_to_numbers(x):\n",
    "    \"\"\"This is a function that converts English to numbers. This function has limited ability.\"\"\"\n",
    "    # remove speical characters, like, \", &, etc.\n",
    "    x = x.replace('\"', \"\")\n",
    "    try:  # Convert string to integer\n",
    "        return int(x)\n",
    "    except:\n",
    "        pass\n",
    "    # Convert intergers written in Engligsh in [-10, 10] to numbers\n",
    "    if x == \"negative ten\":\n",
    "        return -10\n",
    "    if x == \"negative nine\":\n",
    "        return -9\n",
    "    if x == \"negative eight\":\n",
    "        return -8\n",
    "    if x == \"negative seven\":\n",
    "        return -7\n",
    "    if x == \"negative six\":\n",
    "        return -6\n",
    "    if x == \"negative five\":\n",
    "        return -5\n",
    "    if x == \"negative four\":\n",
    "        return -4\n",
    "    if x == \"negative three\":\n",
    "        return -3\n",
    "    if x == \"negative two\":\n",
    "        return -2\n",
    "    if x == \"negative one\":\n",
    "        return -1\n",
    "    if x == \"zero\":\n",
    "        return 0\n",
    "    if x == \"one\":\n",
    "        return 1\n",
    "    if x == \"two\":\n",
    "        return 2\n",
    "    if x == \"three\":\n",
    "        return 3\n",
    "    if x == \"four\":\n",
    "        return 4\n",
    "    if x == \"five\":\n",
    "        return 5\n",
    "    if x == \"six\":\n",
    "        return 6\n",
    "    if x == \"seven\":\n",
    "        return 7\n",
    "    if x == \"eight\":\n",
    "        return 8\n",
    "    if x == \"nine\":\n",
    "        return 9\n",
    "    if x == \"ten\":\n",
    "        return 10\n",
    "    return \"FAIL\"\n",
    "\n",
    "\n",
    "def user(x):\n",
    "    if x == \"FAIL\":\n",
    "        return \"The text cannot be converted to a number.\"\n",
    "    if x < 50:\n",
    "        return \"The number needs to be larger.\"\n",
    "    else:\n",
    "        return \"Success.\"\n",
    "\n",
    "\n",
    "def foobar_text(x):\n",
    "    output = convert_english_to_numbers(x)\n",
    "    if output.data == \"FAIL\":  # This is not traced\n",
    "        return output\n",
    "    else:\n",
    "        return foo(bar(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=negative point one, output=FAIL, feedback=The text cannot be converted to a number.\n",
      "variable=ten, output=380, feedback=Success.\n",
      "Cannot extract suggestion from LLM's response:\n",
      "{\"reasoning\": \"According to the feedback, the execution result was successful. The instructions required improving the output, but given that the feedback is positive with 'Success.', there seems to be no need for changes. The process of converting an English word to a number, applying a negative scaling, performing an addition, and then a multiplication, all follow as per the documentation and produce the specified output successfully. Therefore, there is no need to suggest any changes to the variable values in #Variables.\", \"answer\": \"No changes are needed.\", \"suggestion\": {}}\n",
      "History\n",
      "  0: negative point one\n",
      "  1: ten\n",
      "  2: ten\n"
     ]
    }
   ],
   "source": [
    "GRAPH.clear()\n",
    "x = node(\"negative point one\", trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = [x.data]\n",
    "feedback = \"\"\n",
    "while feedback.lower() != \"Success.\".lower():\n",
    "    output = foobar_text(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "    print(f\"variable={x.data}, output={output.data}, feedback={feedback}\")  # logging\n",
    "    optimizer.step()\n",
    "    history.append(x.data)  # logging\n",
    "\n",
    "print(\"History\")\n",
    "for i, v in enumerate(history):\n",
    "    print(f\"  {i}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of optimizing functions\n",
    "\n",
    "We can use `trace` to optimize python function code directly. This can be achieved by setting `trainable=True` when decorating a custom function by `@bundle`. This would create a `ParameterNode` in the operator, which can be accessed by the `parameter` attribute of the decorated function. It can be used like any other parameters and sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output=2, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return x**2 + 1\n",
      "output=-2, feedback=Success., variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return (x**2 + 1) * -1\n"
     ]
    }
   ],
   "source": [
    "GRAPH.clear()\n",
    "\n",
    "\n",
    "def user(output):\n",
    "    if output < 0:\n",
    "        return \"Success.\"\n",
    "    else:\n",
    "        return \"Try again. The output should be negative\"\n",
    "\n",
    "\n",
    "# We make this function as a parameter that can be optimized.\n",
    "\n",
    "\n",
    "@bundle(trainable=True)\n",
    "def my_fun(x):\n",
    "    \"\"\"Test function\"\"\"\n",
    "    return x**2 + 1\n",
    "\n",
    "\n",
    "x = node(-1, trainable=False)\n",
    "optimizer = FunctionOptimizer([my_fun.parameter], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "feedback = \"\"\n",
    "while feedback != \"Success.\":\n",
    "    output = my_fun(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "\n",
    "    print(f\"output={output.data}, feedback={feedback}, variables=\\n\")  # logging\n",
    "    for p in optimizer.parameters:\n",
    "        print(p.name, p.data)\n",
    "    optimizer.step(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of hyper-parameter optimization for ML models\n",
    "\n",
    "We can use `trace` to optimize the hyper-parameters of a machine learning model using language feedbacks. This example requires `scikit-learn`. Before running the cell below, please ensure that it is installed using:\n",
    "\n",
    "    pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adswamin\\AppData\\Local\\miniconda3\\envs\\newtrace\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_samples = 10000\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=train_samples, test_size=20000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language feedback consists of a text representation of the validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(classifier, guess, history):\n",
    "    score = classifier.score(X_validation, y_validation) * 100\n",
    "    sparsity = np.mean(classifier.coef_ == 0) * 100\n",
    "    return_feedback = f\"\\nScore is the accuracy of the classifier on the validation set, and should be maximized.\"\n",
    "    return_feedback += f\"\\nSparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.\"\n",
    "    return_feedback += f\"By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\"\n",
    "    return_feedback += f\"\\n\\nMost recent guess: \\nRegularization Parameter: {guess:.4f}, Score: {score:.2f}%, Sparsity: {sparsity:.2f}%\"\n",
    "    if len(history) > 0:\n",
    "        return_feedback += f\"\\n\\nHistory of guesses:\"\n",
    "        for item in history:\n",
    "            return_feedback += (\n",
    "                f\"\\nRegularization Parameter: {item[0]:.4f}, Score: {item[1]:.2f}%, Sparsity: {item[2]:.2f}%\"\n",
    "            )\n",
    "    return return_feedback, score, sparsity\n",
    "\n",
    "\n",
    "@bundle(trainable=False)\n",
    "def train_classifier(regularization_parameter):\n",
    "    \"\"\"regularization_parameter is a positive number that controls the sparsity of the classifier. Lower values will increase sparsity, and higher values will decrease sparsity.\"\"\"\n",
    "    classifier = LogisticRegression(C=regularization_parameter, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=0.005, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "variable=0.01, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "variable=0.02, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "variable=0.03, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "variable=0.04, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "variable=0.05, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0500, Score: 87.28%, Sparsity: 31.79%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "variable=0.06, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0600, Score: 87.69%, Sparsity: 30.14%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "Regularization Parameter: 0.0500, Score: 87.28%, Sparsity: 31.79%\n",
      "variable=0.07, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0700, Score: 87.76%, Sparsity: 30.47%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "Regularization Parameter: 0.0500, Score: 87.28%, Sparsity: 31.79%\n",
      "Regularization Parameter: 0.0600, Score: 87.69%, Sparsity: 30.14%\n",
      "variable=0.08, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0800, Score: 87.28%, Sparsity: 25.23%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "Regularization Parameter: 0.0500, Score: 87.28%, Sparsity: 31.79%\n",
      "Regularization Parameter: 0.0600, Score: 87.69%, Sparsity: 30.14%\n",
      "Regularization Parameter: 0.0700, Score: 87.76%, Sparsity: 30.47%\n",
      "variable=0.09, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0900, Score: 87.31%, Sparsity: 25.20%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.23%, Sparsity: 81.63%\n",
      "Regularization Parameter: 0.0100, Score: 85.63%, Sparsity: 61.07%\n",
      "Regularization Parameter: 0.0200, Score: 86.85%, Sparsity: 50.45%\n",
      "Regularization Parameter: 0.0300, Score: 86.94%, Sparsity: 37.76%\n",
      "Regularization Parameter: 0.0400, Score: 87.08%, Sparsity: 34.77%\n",
      "Regularization Parameter: 0.0500, Score: 87.28%, Sparsity: 31.79%\n",
      "Regularization Parameter: 0.0600, Score: 87.69%, Sparsity: 30.14%\n",
      "Regularization Parameter: 0.0700, Score: 87.76%, Sparsity: 30.47%\n",
      "Regularization Parameter: 0.0800, Score: 87.28%, Sparsity: 25.23%\n",
      "Best regularization parameter: 0.07\n",
      "Best score: 87.75500000000001\n"
     ]
    }
   ],
   "source": [
    "x = node(0.005, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = []\n",
    "bestScore = None\n",
    "bestRegularization = None\n",
    "for i in range(10):\n",
    "    classifier = train_classifier(x)\n",
    "    fb, score, sparsity = scorer(classifier.data, x.data, history)\n",
    "    history.append((x.data, score, sparsity))\n",
    "    print(f\"variable={x.data}, feedback={fb}\")  # logging\n",
    "    if bestScore is None or score > bestScore:\n",
    "        bestScore = score\n",
    "        bestRegularization = x.data\n",
    "\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(classifier, fb)\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Best regularization parameter:\", bestRegularization)\n",
    "print(\"Best score:\", bestScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtrace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
